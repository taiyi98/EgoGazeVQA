# In the Eye of MLLM

## Benchmarking Egocentric Video Intent Understanding with Gaze-Guided Prompting

[![Paper](https://img.shields.io/badge/arXiv-2509.07447-b31b1b.svg)](https://arxiv.org/abs/2509.07447)
[![Project Page](https://img.shields.io/badge/Project-Page-green)](https://taiyi98.github.io/projects/EgoGazeVQA)
[![Dataset](https://img.shields.io/badge/ðŸ¤—-Dataset-yellow)](https://huggingface.co/datasets/taiyi09/EgoGazeVQA)

> **Taiying Peng<sup>1</sup>, Jiacheng Hua<sup>2</sup>, Miao Liu<sup>2â€ </sup>, Feng Lu<sup>1â€ </sup>**  
> <sup>1</sup>State Key Laboratory of VR Technology and Systems, School of CSE, Beihang University  
> <sup>2</sup>College of AI, Tsinghua University  
> **NeurIPS D&B 2025**

---

## Overview

This repository provides the official code for **EgoGazeVQA**, a benchmark for evaluating multimodal large language models (MLLMs) on egocentric video understanding tasks with gaze guidance. 

**Code Purpose:**
- Generate gaze-guided QA pairs from egocentric videos (spatial, temporal, causal intent questions)
- Evaluate MLLMs with three gaze-guided prompting strategies (textual, visual marks, salience maps)
- Calculate and analyze model performance on intent understanding tasks



## Timeline

- **[2025-10]** Code and dataset publicly released
- **[2025-09]** Paper accepted at NeurIPS 2025 D&B Track



## TODO

- [ ] Add fine-tuning scripts for LoRA adaptation



## Repository Structure

```
EgoGazeVQA/
â”œâ”€â”€ generate_tool/
â”‚   â”œâ”€â”€ auto.sh
â”‚   â”œâ”€â”€ spatial.py
â”‚   â”œâ”€â”€ temporal.py
â”‚   â”œâ”€â”€ causal.py
â”‚   â””â”€â”€ create_datasets.py
â”œâ”€â”€ test_tool/
â”‚   â”œâ”€â”€ qwenvl_test/
â”‚   â”‚   â”œâ”€â”€ test_wo.py
â”‚   â”‚   â”œâ”€â”€ test_gaze.py
â”‚   â”‚   â”œâ”€â”€ test_mark.py
â”‚   â”‚   â””â”€â”€ test_saliencemap.py
â”‚   â”œâ”€â”€ prompt_gazees/
â”‚   â”œâ”€â”€ multiframes/
â”‚   â”œâ”€â”€ gaze_trajectory.py
â”‚   â””â”€â”€ caculate.py
```



## Installation

```bash
git clone https://github.com/taiyi98/EgoGazeVQA.git
cd EgoGazeVQA

conda create -n egogazevqa python=3.10
conda activate egogazevqa

pip install -r requirements.txt
```



## Dataset

Download from [ðŸ¤— Hugging Face](https://huggingface.co/datasets/taiyi09/EgoGazeVQA):

```bash
huggingface-cli download taiyi98/EgoGazeVQA --repo-type dataset --local-dir ./data
```



## Usage

### Generate QA Pairs

```bash
cd generate_tool

# Generate for specific video
python spatial.py --video_id <VIDEO_ID> --target_index <INDEX>
python temporal.py --video_id <VIDEO_ID> --target_index <INDEX>
python causal.py --video_id <VIDEO_ID> --target_index <INDEX>

# Batch processing
bash auto.sh
```

### Evaluate Models

```bash
cd test_tool/qwenvl_test

python test_wo.py              # Baseline (no gaze)
python test_gaze.py            # Textual gaze prompt
python test_mark.py            # Visual gaze marks
python test_saliencemap.py     # Gaze salience maps
```

### Calculate Results

```bash
cd test_tool
python caculate.py --result_file <RESULT_CSV_PATH>
```


## Citation

```bibtex
@misc{peng2025eyemllmbenchmarkingegocentric,
    title={In the Eye of MLLM: Benchmarking Egocentric Video Intent Understanding with Gaze-Guided Prompting}, 
    author={Taiying Peng and Jiacheng Hua and Miao Liu and Feng Lu},
    year={2025},
    eprint={2509.07447},
    archivePrefix={arXiv},
    primaryClass={cs.CV},
    url={https://arxiv.org/abs/2509.07447}
}
```

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.
